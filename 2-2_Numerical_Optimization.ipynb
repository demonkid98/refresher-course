{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=15%><img src=\"./img/UGA.png\"></img></td>\n",
    "<td><center><h1>Refresher Course on Matrix Analysis and Optimization</h1></center></td>\n",
    "<td width=15%><a href=\"http://www.iutzeler.org\" style=\"font-size: 16px; font-weight: bold\">Franck Iutzeler</a> <a href=\"https://ljk.imag.fr/membres/Jerome.Malick/\" style=\"font-size: 16px; font-weight: bold\">Jérôme Malick</a><br/> 2017/2018 </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><div id=\"top\"></div>\n",
    "\n",
    "<center><a style=\"font-size: 40pt; font-weight: bold\">Chap. 2 - Refresher Course </a></center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "# ``2. Numerical Optimization``\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#style\"><b>Package check and Styling</b></a><br/><br/><b>Outline</b><br/><br/>\n",
    "&nbsp;&nbsp;&nbsp; a) <a href=\"#OGrad\"> the Gradient Method </a><br/>&nbsp;&nbsp;&nbsp; b) <a href=\"#ORef\"> Application to Regression </a><br/>&nbsp;&nbsp;&nbsp; c) <a href=\"#OAda\"> an Adaptive Gradient Algorithm </a><br/>&nbsp;&nbsp;&nbsp; d) <a href=\"#OCla\"> Application to Classification </a><br/>&nbsp;&nbsp;&nbsp; e) <a href=\"#ONew\"> Newton's Algorithm </a><br/>&nbsp;&nbsp;&nbsp; f) <a href=\"#OFur\"> To go Further </a><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"OGrad\"> a) the Gradient Method </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "\n",
    "We consider two $\\mathbb{R}^2 \\to \\mathbb{R}$ convex functions with the same global minimizer $(3,1)$ but quite different *shapes* and see how this impacts the performance of gradient-based algorithms. The considered functions $f$ and $g$ and their 3D are:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>\\begin{array}{rrcll}\n",
    "f: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & 4 (x_1-3)^2 + 2(x_2-1)^2\n",
    "\\end{array}</th>\n",
    "<th> \\begin{array}{rrcll}\n",
    "g: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & \\log( 1 + \\exp(4 (x_1-3)^2 ) + \\exp( 2(x_2-1)^2 ) ) - \\log(3) .\n",
    "\\end{array} </th>\n",
    "</tr>\n",
    "<td> <img src=\"img/simple.png\" alt=\"f\" />  </td>\n",
    "<td> <img src=\"img/harder.png\" alt=\"f\" /> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For convenience, we provide custom plotting functions in <tt>lib/custom_plot_lib.py</tt>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.custom_plot_lib import *\n",
    "import numpy as np\n",
    "# from math import log, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Observe how is written the function <tt>f</tt> that return $f(x)$ from input vector $x$. Observe the 3D plot and level plot with <tt>custom_3dplot...</tt> and <tt>level_plot...</tt>. Do the same for  function <tt>g</tt>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Function f\n",
    "    \"\"\"\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return 4*(x1-3)**2+2*(x2-1)**2\n",
    "\n",
    "f_plot_param  = {'x1_min' : -0.5, 'x1_max' : 5.5,\n",
    "                 'x2_min' : -0.5, 'x2_max' : 5.5,\n",
    "                 'nb_points' : 200,\n",
    "                 'v_min' : 0, 'v_max' : 80, 'levels' : [0.5,1,2,5,10,15],\n",
    "                 'title' : 'f: a simple function' }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_3dplot( f, f_plot_param )\n",
    "level_plot( f, f_plot_param )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return np.log(1 + np.exp(4 * (x1 - 3)**2) + np.exp(2 * (x2 - 1)**2)) - np.log(3)\n",
    "\n",
    "g_plot_param  = {'x1_min' : -0.5, 'x1_max' : 5.5,\n",
    "                 'x2_min' : -0.5, 'x2_max' : 5.5,\n",
    "                 'nb_points' : 500,\n",
    "                 'v_min' : 0, 'v_max' : 100, 'levels' : [0.5,1,2,5,10,15],\n",
    "                 'title' : 'f: a harder function' }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_3dplot( g, g_plot_param )\n",
    "level_plot( g, g_plot_param )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b>  Fill the function <tt>f_grad</tt> that return $\\nabla f(x)$ from input vector $x$. Do the same for  function <tt>g_grad</tt>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_grad(x): # ...................................\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    g = np.array( [ 8 * x1 - 24, 4 * x2 - 4  ] )\n",
    "    return g\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g_grad(x): # ...................................\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    denom = 1 / (1 + np.exp(4 * (x1 - 3)**2) + np.exp(2 * (x2 - 1)**2))\n",
    "    g1 = denom * (8 * x1 - 24) * np.exp(4 * (x1 - 3)**2)\n",
    "    g2 = denom * (4 * x2 - 4) * np.exp(2 * (x2 - 1)**2)\n",
    "    g = np.array( [ g1, g2  ] )\n",
    "    return g\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b>  implement a constant stepsize gradient method <tt>gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX )</tt> that takes:\n",
    "<ul>\n",
    "<li> <tt>f</tt> and <tt>f_grad</tt>: respectively functions and gradient simulators;</li>\n",
    "<li> <tt>x0</tt>: starting point;</li>\n",
    "<li> <tt>step</tt>: a stepsize;</li>\n",
    "<li> <tt>PREC</tt> and <tt>ITE_MAX</tt>: stopping criteria for sought precision and maximum number of iterations;</li>\n",
    "</ul>\n",
    "\n",
    "and return <tt>x</tt>, the final value, and <tt>x_tab</tt>, the matrix of all vectors stacked vertically.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX ): # .......................................\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "\n",
    "    i = 0\n",
    "    fx_prev = f(x)\n",
    "    while i < ITE_MAX:\n",
    "        x = x - step * f_grad(x)\n",
    "        fx = f(x)\n",
    "        x_tab = np.vstack((x_tab, x))\n",
    "\n",
    "        if np.abs(fx - fx_prev) < PREC:\n",
    "            break\n",
    "        fx_prev = fx\n",
    "        i += 1\n",
    "        print(i, end=' ')\n",
    "\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 4:</b> Test your gradient descent function on $f$ and $g$: i) Verify that the final point is close to the sought minimizer $(3,1)$; ii) observe the behavior of the iterates with <tt>level_points_plot</tt>. Change the stepsize and give the values for which the algorithm (i) diverges and (ii) oscillates. Compare with theoretical limits by computing the Lipschitz constant of the gradients.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =  .1 # .........................\n",
    "PREC = 1e-5 # .........................\n",
    "ITE_MAX = 100 # .........................\n",
    "x0 = np.array([0,0]) # .........................\n",
    "\n",
    "x,x_tab = gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX )\n",
    "level_points_plot( f , x_tab , f_plot_param )\n",
    "\n",
    "### 1 -> diverge\n",
    "### .2 -> oscillate a bit, but get there after 15 iter\n",
    "### .1 -> converge, 12 iter\n",
    "### .01 -> converge slowly\n",
    "### .001 -> converge too slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =  .2 # .........................\n",
    "PREC = 1e-5 # .........................\n",
    "ITE_MAX = 200 # .........................\n",
    "x0 = np.array([0,0]) # .........................\n",
    "\n",
    "x,x_tab = gradient_algorithm(g , g_grad , x0 , step , PREC , ITE_MAX )\n",
    "level_points_plot( g , x_tab , g_plot_param )\n",
    "### .3 -> diverge\n",
    "### .2 -> oscillate a bit, 18 iter only\n",
    "### .1 -> converge, 32 iter\n",
    "### .01 -> converge slowly, but will get there\n",
    "### .001 -> converge too slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ORef\"> b) Application to Regression </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "\n",
    "We now get back to the problem of predicting the final grade of a student from various features treated in the Matrix part of the course.\n",
    "\n",
    "We remind that mathematically, from the $m_{learn}  \\times (n+1)$  *learning matrix* $A_{learn}$\n",
    "($m_{learn} = 300$, $n=27$)  comprising of the features values of each training student in line, and the vector of the values of the target features $b_{learn}$;  we seek a size-$(n+1)$ *regression vector* that minimizes the squared error between  $A_{learn} x$ and $b_{learn}$. This problem boils down to the following least square problem:\n",
    "$$ \\min_{x\\in\\mathbb{R}^{n+1}} s(x) =  \\frac{1}{2} \\|  A_{learn} x - b_{learn} \\|_2^2 . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Construct the suitable function $s$ and gradient simulator as in the previous section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s(A, x, b):\n",
    "#     print('s', A.shape, x.shape, b.shape)\n",
    "    lc = np.dot(A, x) - b\n",
    "    return 1 / 2 * np.dot(lc.T, lc)\n",
    "\n",
    "def s_grad(A, x, b):\n",
    "#     print('s_grad', A.shape, x.shape, b.shape)\n",
    "    return np.dot(A.T, np.dot(A, x) - b)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b> Compute the Lipschitz constant of the gradient of $s$. Find a solution to the minimization of $s$ using your gradient algorithm. Compare with Numpy's Least Square routine.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Ls_grad(A):\n",
    "    return np.linalg.norm(np.dot(A.T, A))\n",
    "\n",
    "def grad_algo_linear(f, f_grad, A, x0, b, step, PREC, ITE_MAX ): # .......................................\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "\n",
    "    i = 0\n",
    "    fx_prev = f(A, x, b)\n",
    "    while i < ITE_MAX:\n",
    "        gr = f_grad(A, x, b)\n",
    "        x = x - step * gr\n",
    "        \n",
    "        fx = f(A, x, b)\n",
    "        x_tab = np.vstack((x_tab, x))\n",
    "\n",
    "        if np.abs(fx - fx_prev) < PREC:\n",
    "            break\n",
    "        fx_prev = fx\n",
    "        i += 1\n",
    "#         print(i, end=' ')\n",
    "\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_file = np.load('data/student.npz')\n",
    "A_learn = dat_file['A_learn']\n",
    "b_learn = dat_file['b_learn']\n",
    "A_test = dat_file['A_test']\n",
    "b_test = dat_file['b_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =  2 / Ls_grad(A_learn)\n",
    "PREC = 1e-5\n",
    "ITE_MAX = 100\n",
    "\n",
    "x0 = np.random.rand(A_learn.shape[1])\n",
    "\n",
    "x, x_tab = grad_algo_linear(s, s_grad, A_learn, x0, b_learn, step, PREC, ITE_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_reg = np.linalg.lstsq(A_learn,b_learn)[0]\n",
    "\n",
    "pred = np.dot(A_test, x)\n",
    "pred_reg = np.dot(A_test, x_reg)\n",
    "\n",
    "for i in range(pred.size):\n",
    "    print(pred[i], pred_reg[i], b_test[i])\n",
    "\n",
    "### similar results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b> Generate a random Gaussian matrix/vector couple $A,b$ with increasing size. Create simulators to compare the execution time of constant stepsize gradien and pseudo-inverse computation \\emph{via} SVD on the least squares problem $\\min_x \\|Ax-b\\|_2^2$. Notably change the *shape* of $A$ from *tall* (nb. of rows $>\\!>$ nb. of cols.) to *fat* (nb. of rows $<\\!<$ nb. of cols.).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(m, n):\n",
    "    A = np.random.normal(size=(m, n))\n",
    "    b = np.random.normal(size=(m,))\n",
    "    return A, b\n",
    "\n",
    "def solve_svd(A, b):\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    A_pinv_svd  = np.matmul(np.matmul(Vt.T, np.diag(1 / s)), U.T)\n",
    "    return np.matmul(A_pinv_svd, b)\n",
    "\n",
    "A, b = generate_data(100000, 10)\n",
    "x0 = np.random.rand(A.shape[1])\n",
    "step =  2 / Ls_grad(A)\n",
    "\n",
    "\n",
    "\n",
    "%timeit -n 1000 grad_algo_linear(s, s_grad, A, x0, b, step, PREC, ITE_MAX)\n",
    "%timeit -n 1000 np.linalg.lstsq(A, b)\n",
    "%timeit -n 1000 solve_svd(A, b)\n",
    "\n",
    "### -- (m, n) = (10000, 10)\n",
    "# grad: 1.4 ms ± 35 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "# linear solve: 498 µs ± 8.83 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "# svd: 632 µs ± 14.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "\n",
    "### -- (m, n) = (10, 10000)\n",
    "# grad: 1.09 ms ± 74.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "# linear solve: 719 µs ± 9.24 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "# svd: 1.53 ms ± 14.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"OAda\"> c) an Adaptive Gradient Algorithm </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "The *Rosenbrock* function $r$ writes \n",
    "<table>\n",
    "<tr>\n",
    "<th>\\begin{array}{rrcll}\n",
    "r: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  (1-x_1)^2 + 100(x_2-x_1^2)^2 .\n",
    "\\end{array}</th>\n",
    "</tr>\n",
    "<td> <img src=\"img/rosenbrock.png\" alt=\"f\" />  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Fill the functions <tt>r</tt> that return $r(x)$ from input vector $x$; and <tt>r_grad</tt> that return $\\nabla r(x)$ from input vector $x$. Observe the 3D plot and level plot of the function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Definition of function r\n",
    "def r(x):\n",
    "    \"\"\" Rosenbrock.\"\"\"\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return (1-x1)**2+100*(x2-x1**2)**2\n",
    "\n",
    "\n",
    "\n",
    "r_plot_param  = {\n",
    "                 'x1_min' : -1.5, 'x1_max' : 1.55,\n",
    "                 'x2_min' : -0.2, 'x2_max' : 1.5,\n",
    "#                  'x1_min' : .5, 'x1_max' : 1.,\n",
    "#                  'x2_min' : .5, 'x2_max' : 1.,\n",
    "                 'nb_points' : 200,\n",
    "                 'v_min' : 0, 'v_max' : 120, 'levels' : [0.05,1,5,15,50,200],\n",
    "                 'title' : 'r: Rosenbrock function' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_3dplot( r,r_plot_param)\n",
    "level_plot( r, r_plot_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r_grad(x): # .........................\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    g1 = 2 * x1 - 2 - 400 * x1 * (x2 - x1**2)\n",
    "    g2 = 200 * (x2 - x1**2)\n",
    "    return np.array( [g1, g2] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b> Try to minimize $r$ using your constant stepsize gradient function <tt>gradient_algorithm</tt>. Can you find a stepsize for which the algorithm converges?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =  .001 # .........................\n",
    "PREC = 1e-5 # .........................\n",
    "ITE_MAX = 200 # .........................\n",
    "x0 = np.array([0,0]) # .........................\n",
    "\n",
    "x,x_tab = gradient_algorithm(r , r_grad , x0 , step , PREC , ITE_MAX )\n",
    "level_points_plot( r , x_tab , r_plot_param )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b> Implement an *adaptive* stepsize gradient method <tt>gradient_adaptive_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX )</tt> that takes the same inputs and returns the same as the gradient method but implements a *stepsize adaptation method*. For instance, one can use this rule:\n",
    "\\begin{align*}\n",
    "\\mathbf{if} f(x_{k+1})>f(x_k)&:\\\\\n",
    "x_{k+1} &= x_k\\\\\n",
    "step &= step/2\n",
    "\\end{align*}\n",
    "which halves the stepsize if a gradient step makes the functional value increase.<br/>\n",
    "\n",
    "Test your method on $r$: i) Verify that the final point is close to the sought minimizer $(1,1)$; ii) observe the behavior of the iterates with <tt>level_points_plot</tt>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =  .07 # .........................\n",
    "PREC = 1e-6 # .........................\n",
    "ITE_MAX = 200 # .........................\n",
    "x0 = np.array([0,0]) # .........................\n",
    "decay = 2\n",
    "\n",
    "def gradient_adaptive_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX ): # .......................................\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "\n",
    "    i = 0\n",
    "    fx_prev = f(x)\n",
    "    while i < ITE_MAX:\n",
    "        gr = f_grad(x)\n",
    "#         print(gr, end=', ')\n",
    "        x = x - step * gr\n",
    "        fx = f(x)\n",
    "        if fx > 1e3:\n",
    "            print('oor')\n",
    "            break\n",
    "        x_tab = np.vstack((x_tab, x))\n",
    "\n",
    "        if np.abs(fx - fx_prev) < PREC:\n",
    "            break\n",
    "\n",
    "        if fx > fx_prev:\n",
    "            step /= decay\n",
    "            print(step, end=' ')\n",
    "        fx_prev = fx\n",
    "        i += 1\n",
    "\n",
    "    return x,x_tab\n",
    "\n",
    "x,x_tab = gradient_adaptive_algorithm(r , r_grad , x0 , step , PREC , ITE_MAX )\n",
    "level_points_plot( r , x_tab , r_plot_param)\n",
    "print(x, r(x), r_grad(x))\n",
    "\n",
    "### step=.01, decay=2: [ 0.65800501  0.43132033] 0.117232909954\n",
    "### step=.05, decay=2: [ 0.65185387  0.4232292 ] 0.121489403681\n",
    "# TODO figure this out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"OCla\"> d) Application to Classification </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "\n",
    "Binary classification is another popular problem in machine learning. Instead of predicting a numerical value, the goal is now to classify the student into two classes: $+1$ -- *pass* i.e. final grade $\\geq 10$; and $-1$ -- *fail*. To this purpose, we create a class vector $c_{learn}$  from the observation vector $b_{learn}$ by simply setting $c_{learn}(i) = +1 $ if  $b_{learn}(i)\\geq10$ and $-1$ otherwise. Then, the most common approach is to minimize the logistic loss regularized by a squared norm:\n",
    "\\begin{equation}\n",
    "\\min_{x\\in\\mathbb{R}^{n+1}}  \\ell(x) = \\sum_{i=1}^{m_{learn} } \\log\\left( 1 + \\exp\\left( -c_{learn}(i) a_i^{\\mathrm{T}}  x   \\right) \\right)  + \\frac{1}{m}\\|x\\|^2\n",
    "\\end{equation}\n",
    "where $a^\\mathrm{T}_i$ is the $i$-th row of $A_{learn}$.\n",
    "\n",
    "Then, from a solution $x^\\star$ of this problem, one can classify a new example, represented by its feature vector $a$, as such: the quantity $p(a) = \\frac{1}{1+\\exp(- a^\\mathrm{T} x^\\star)}$ estimates the probability of belonging to class $1$; thus, one can decide class $+1$ if for instance $ p(a) \\geq 0.5$; otherwise, decide class $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Compute the gradient of $q(t)  = \\log(1+\\exp(t))$. Is the function is convex? Deduce that $\\ell$ is convex and its gradient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b> Construct the suitable function and gradient simulators in order to use your <tt>gradient_adaptive_algorithm</tt> to minimize $\\ell$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b> From a final point of the optimization algorithm above, generate a decision vector corresponding to the testing set $A_{test}$. Evaluate the classification error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ONew\"> e) Newton's Algorithm </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Fill the function <tt>f_grad_hessian</tt> that return $\\nabla f(x)$ and $H_f(x)$ from input vector $x$. Same thing for $g$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_grad_hessian(x): # ...................................\n",
    "    g = np.array( [ 0.0  ,  0.0  ] )\n",
    "    H = np.array(  [ ( 0.0 , 0.0 )  ,  ( 0.0 , 0.0 )  ]  )\n",
    "    return g,H\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g_grad_hessian(x): # ...................................\n",
    "    g = np.array( [ 0.0  ,  0.0  ] )\n",
    "    H = np.array(  [ ( 0.0 , 0.0 )  ,  ( 0.0 , 0.0 )  ]  )\n",
    "    return g,H\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b> Implement Newton's method in <tt>newton_algorithm(f , f_grad_hessian , x0 , PREC , ITE\\_MAX )</tt> that takes as an input:\n",
    "<ul>\n",
    "<li> <tt>f</tt> and <tt>f_grad_hessian</tt>: respectively functions and gradient + Hessian simulators;\n",
    "<li> <tt>x0</tt>: starting point;</li>\n",
    "<li> <tt>PREC</tt> and <tt>ITE_MAX</tt>: stopping criteria for sought precision and maximum number of iterations;</li>\n",
    "</ul>\n",
    "<tt>x</tt> the final value, and <tt>x_tab</tt>, the matrix of all vectors stacked vertically.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton_algorithm(f , f_grad_hessian , x0 , PREC , ITE_MAX ): # .......................................\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b> Test your method on $f$ and $g$: i) Verify that the final point is close to the sought minimizer $(3,1)$; ii) observe the behavior of the iterates with <tt>level_points_plot</tt>.  \n",
    "\n",
    "\n",
    "Compare graphically constant stepsize gradient and Newton's algorithms with <tt>level_2points_plot</tt>. <br/>\n",
    "\n",
    "\n",
    "Newton's algorithm should take exactly one iteration to converge for function $f$. Why so? Is it the case for function $g$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 4:</b> Compare Newton method with the adaptive gradient in the case of the classification problem\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"OFur\"> f) To go Further </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "\n",
    "We introduce two functions:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>\\begin{array}{rrcll}\n",
    "t: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & (0.6 x_1 + 0.2 x_2)^2 \\left((0.6 x_1 + 0.2 x_2)^2 - 4 (0.6 x_1 + 0.2 x_2)+4\\right) \\\\\n",
    "&            &         & + (-0.2 x_1 + 0.6 x_2)^2\n",
    "\\end{array}</th>\n",
    "<th> \\begin{array}{rrcll}\n",
    "p: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  \\left| x_1-3 \\right|  + 2\\left| x_2-1\\right| \n",
    "\\end{array} </th>\n",
    "</tr>\n",
    "<td> <img src=\"img/poly.png\" alt=\"f\" />  </td>\n",
    "<td> <img src=\"img/two_pits.png\" alt=\"f\" /> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"exo\"> <b>Question:</b> Test adaptive gradient methods on these functions from different starting points. What do you observe?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div id=\"style\"></div>\n",
    "### Package Check and Styling\n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lib.notebook_setting as nbs\n",
    "\n",
    "packageList = ['IPython', 'numpy', 'scipy', 'matplotlib', 'cvxopt']\n",
    "nbs.packageCheck(packageList)\n",
    "\n",
    "nbs.cssStyling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
